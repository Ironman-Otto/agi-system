Architecture – Question Template Schema (v1)
1. Purpose and Scope
This document defines the Question Template Schema used by the Question Generation and Curiosity Subsystem. It is a standalone architecture specification and can be included directly in the master architecture document.
The schema answers:
What is a “question template” in this architecture?
How are templates categorized by objectives and domains?
How does the system select templates under constraints?
How do templates evolve over time through learning?
This document focuses on:
Template data model (fields and meaning)
Taxonomy and tagging rules
Template scoring and selection signals
Template lifecycle (creation, update, retirement)
Logging requirements for learning

2. Design Principles
2.1 Templates Enable Fast, Directed Questioning
The system should not synthesize questions from scratch in most cases. Instead, it should select from reusable templates and only generate novel questions when templates are insufficient.
2.2 Templates Are Objective-Linked
Every template must be tied to at least one objective category (e.g., Safety, Quality). This prevents undirected questioning.
2.3 Templates Are Domain-Tagged
Templates must be tagged so selection can be specialized across: - directives - perception events - planning - artifacts - execution and debugging
2.4 Templates Must Support Termination
Templates should include expectations about what constitutes a satisfactory answer so that termination logic can assess progress.

3. Definitions
3.1 Question Template
A Question Template is a reusable pattern that generates a concrete question (internal or external) when bound to context variables.
3.2 Concrete Question
A Concrete Question is an instantiated template with bound variables, asked during a question episode.
3.3 Template Binding
Binding is the process of taking a template and substituting context values (entity, location, artifact type, threshold, etc.).

4. Template Categories (v1)
Templates are categorized by objective alignment and operational domain.
4.1 Objective Categories (Primary)
Templates must declare one or more objective categories from the Objective Taxonomy:
Tier 0: Safety / Hazard
Tier 1: Policy / Values / Permissions
Tier 2: Task Completion
Tier 3: Quality (Correctness / Completeness)
Tier 4: Efficiency (Cost / Time)
Tier 5: Learning / Curiosity
4.2 Operational Domains (Secondary)
Templates must declare one or more domains:
Perception – sensory anomalies, environment analysis
Directive Understanding – interpret/clarify user intent
Planning – derive steps, constraints, dependencies
Execution – readiness checks, preconditions, safety gates
Artifact QA – evaluate produced artifacts
Diagnostics – failures, exceptions, transport/log issues
Value Alignment – permission checks and policy compliance

5. Question Template Schema (Logical Model)
The following fields represent a practical v1 schema.
5.1 Core Identity
template_id – unique identifier
name – short human-friendly name
version – semantic version or integer version
status – active | deprecated | retired
5.2 Intent and Objective Linkage
objective_tags – list of objective categories (required)
domain_tags – list of operational domains (required)
5.3 Question Form
question_text – the template string with placeholders
placeholders – list of required variables (names + types)
Example placeholders: - subject_entity (string) - location_hint (string) - threshold (number) - time_window (string)
5.4 Question Type and Expected Answer
question_type – what kind of inquiry this is
identification
localization
threat_assessment
parameter_request
verification
completeness_check
correctness_check
estimation
decision_support
expected_answer_type – the type of answer expected
boolean
categorical
numeric
textual
structured
answer_schema (optional) – for structured answers (fields)
5.5 Preconditions and Applicability
applicability_conditions – rules describing when it applies
e.g., requires domain_context
e.g., requires artifact_type
exclusions – rules describing when it should not be used
e.g., if safety objective active, exclude learning templates
5.6 Value and Cost Metadata
expected_information_gain – low/medium/high (or numeric)
expected_cost – low/medium/high (or numeric)
risk_sensitivity – none/low/medium/high
These fields support selection under constraints.
5.7 Historical Utility (Learning Signals)
utility_score – aggregated usefulness score
success_count – number of times it helped achieve objective
failure_count – number of times it was unhelpful
avg_uncertainty_reduction – estimated
These fields should be updated from episode logs.

6. Template Instantiation and Binding
6.1 Binding Inputs
Templates bind using a Context Snapshot containing: - active objectives - directive intent - environment hints - artifact metadata - execution state - constraints
6.2 Binding Output
The binding process outputs: - concrete_question_id - question_text (resolved) - objective linkage - expected answer type/schema

7. Selection and Scoring (How Templates Are Chosen)
The Question Selector should score candidate templates using at least:
Objective Priority – higher tier objectives dominate
Relevance – match between context and template tags
Expected Information Gain – higher gain favored
Expected Cost – lower cost favored when time/cost constrained
Risk Sensitivity – safety/policy templates rise under risk
Historical Utility – templates that worked before are favored
This produces a ranked list from which a small initial set is selected.

8. Standard “Innate” Starter Templates (v1 Set)
A small starter library is recommended for early implementation.
8.1 Safety / Threat Templates
“What is the source of {anomaly}?”
“Is {anomaly} dangerous?”
“How close is {anomaly}?”
“What is the worst-case impact of {anomaly}?”
8.2 Clarification Templates (Missing Parameters)
“What constraints apply to {task}?”
“What is the target environment for {task}?”
“What output format is required for {artifact}?”
8.3 Quality Assurance Templates
“Does this output meet all stated objectives?”
“What is missing from this artifact?”
“Is there any internal inconsistency?”
8.4 Execution Readiness Templates
“Do I have permissions to execute {behavior}?”
“What are the preconditions for {behavior}?”
8.5 Learning Templates
“What new template would have reduced uncertainty faster?”
“What pattern should be remembered for next time?”
These templates should be tagged and scored as described above.

9. Template Lifecycle
9.1 Creation
Templates may be created by: - manual authoring (initial phase) - learning from successful question episodes - LLM-assisted proposal (future), subject to validation
9.2 Update
Templates should be updated when: - utility score trends downward - context patterns change - new answer schema is required
9.3 Deprecation and Retirement
Templates may be deprecated if: - redundant - consistently low utility - unsafe or too broad
Retired templates remain in history for analysis but are not used.

10. Logging Requirements
To support learning, each question episode should log:
template_id and version
binding variables used
answer received (type + value)
impact assessment (did it change decision?)
uncertainty reduction estimate
objective satisfaction changes
These logs are the primary data source for maintaining template utility scores.

11. Integration Hooks
The template system must integrate with:
Objective Manager (objective tags)
Context Model (binding)
Question Selector (scoring)
Termination Controller (satisfactory answer expectations)

12. Next Architectural Step
The next document in sequence is:
Architecture – Termination Metrics and Inquiry Budgets (v1)
This will define the termination logic for question episodes and how time/risk/value budgets constrain questioning.

13. Conclusion
The Question Template Schema provides a reusable, objective-linked mechanism for bounded, directed inquiry. By combining tagging, applicability rules, expected answer types, and historical utility signals, the system can select a small set of high-value questions rather than exploring indefinitely.